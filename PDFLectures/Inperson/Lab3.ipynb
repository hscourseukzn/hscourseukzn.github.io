{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Lab3.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xa7m03wXDqPr"},"source":["# Lab #3\n","\n","Today will be an introduction to coding convolutional neural networks (CNNs)."]},{"cell_type":"markdown","metadata":{"id":"p4uAxpgMEiON"},"source":["## Question 1: Classifying Digits\n","Let's revisit the MNIST dataset and build a CNN classifier.\n","\n","Load the MNIST dataset provided by keras. This contains 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. Split the data into training and testing sets."]},{"cell_type":"code","metadata":{"id":"kj86c3dODAq0","executionInfo":{"status":"ok","timestamp":1617992447122,"user_tz":240,"elapsed":3019,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Import needed packages\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow import keras\n","import pandas as pd\n","from tensorflow.keras.datasets import reuters\n","from keras.utils import to_categorical\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiRdPoMpDy4M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617992447669,"user_tz":240,"elapsed":3552,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}},"outputId":"889b47c7-b029-4fff-aa84-e3eaf881cec2"},"source":["# Load the data\n","from keras.datasets import mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zGEP12vLEuoD"},"source":["Print the shape of the training and testing datasets."]},{"cell_type":"code","metadata":{"id":"fT4zNbguEuw-","executionInfo":{"status":"ok","timestamp":1617992447670,"user_tz":240,"elapsed":3543,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dc5o3N2KE1My"},"source":["Let's reshape the data to fit the keras format. Don't worry too much about this chunk for now.\n","\n"]},{"cell_type":"code","metadata":{"id":"rnMbxh6hE1Wm","executionInfo":{"status":"ok","timestamp":1617992447672,"user_tz":240,"elapsed":3542,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["from keras import backend as K\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n","    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n","    input_shape = (1, 28, 28)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","    input_shape = (28, 28, 1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeAWfpJjE1d0"},"source":["Now print the shape again to see what changed.\n","\n"]},{"cell_type":"code","metadata":{"id":"TA81AvJvE1lf","executionInfo":{"status":"ok","timestamp":1617992447673,"user_tz":240,"elapsed":3534,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6pwQ-W7E1s_"},"source":["Question 2 in Homework 1 asks you to train a neural network on the Boston housing data. This dataset contains features on very different scales (for example there are both binary features and real-valued features). While the MNIST features take on values between 0 and 1 and do not need to be normalized, we will go through the exercise of normalizing the values before training our network.\n","\n","Can you think of other algorithms in which normalization is necessary? Is it necessary in the case of neural networks? Why or why not?\n","\n","- Clustering, PCA, random forest, etc. Not necessary (universal function approximator) but makes training easier in cases in which the features have very different scales. \n","\n","\n","Normalize the data. Be sure to normalize the test set with the training set mean and standard deviation. Don't forget to convert the training and testing sets to `float32`.\n"]},{"cell_type":"code","metadata":{"id":"rDAgfzT6E11S","executionInfo":{"status":"ok","timestamp":1617992447673,"user_tz":240,"elapsed":3531,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Normalize data\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5trWQL2FjCr"},"source":["How will the code above need to be changed for Boston housing dataset? Why?\n","\n","- Need to calculate mean and standard deviation per feature, thus need to use something like `x_train.mean(axis=0)`.\n","\n","Before we define and fit our model let's one-hot encode the labels. Don't forget to do the same for the testing labels and note you will not need to do this step in the case of regression."]},{"cell_type":"code","metadata":{"id":"RthqtOwJFjLa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617992447674,"user_tz":240,"elapsed":3524,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}},"outputId":"36d61510-493b-491c-f9ec-30ab46032cd7"},"source":["y_train.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000,)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"RK8e9gKdFqdU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617992447675,"user_tz":240,"elapsed":3514,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}},"outputId":"0e9c4fcb-2264-4505-c6bd-a4659a7d361c"},"source":["y_train = tf.keras.utils.to_categorical(y_train, 10)\n","y_test = tf.keras.utils.to_categorical(y_test, 10)\n","y_train.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 10)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"cMFizDPkFvLx"},"source":["Now fit a shallow convolutional neural network with a single `Dense` layer. Include one convilutional layer with 32 convolutional filters of size 3x3 and use the `relu` activation function.\n","\n","After the convolutional layer, flatten the tensor to be fed into the `Dense` layer.\n","\n","In the `Dense` layer use enough output nodes to have one corresponding to each class label (10). What is the activation function you should use here?\n","\n","In the optimizer use the `Adadelta` optimization function, and choose an appropriate loss function and model performance measure.\n","\n","Run the network for 5 epochs and use a `batch_size` of 64."]},{"cell_type":"code","metadata":{"id":"tHp8woWEFvWF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617992447857,"user_tz":240,"elapsed":3694,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}},"outputId":"ad663a0d-cada-4d84-f73d-1f876872e8f2"},"source":["# Define model\n","model = keras.Sequential([\n","                          layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=\"input\")\n","                          layers.Flatten(),\n","                          layers.Dense(64,activation='relu')\n","                          layers.Dense(10, activation='softmax')\n","                          ])\n","\n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer = tf.keras.optimizers.Adadelta(),\n","              metrics = ['accuracy'])\n","\n","model.fit(x_train, y_train, batch_size = 64, epochs = 5, verbose = 1)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Model.fit of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fbd93076790>>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"LWt8-hkWFz_4"},"source":["Report the test set accuracy.\n","\n"]},{"cell_type":"code","metadata":{"id":"Zl-DUP-aF0Ld","executionInfo":{"status":"ok","timestamp":1617992447858,"user_tz":240,"elapsed":3684,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here\n","test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n","print(test_acc)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_nR10umY7Cn"},"source":["## Question 2: Classifying Dogs and Cats\n","The cats vs. dogs dataset that we will use isn't packaged with Keras. It was made available by Kaggle.com as part of a computer vision competition in late 2013, back when convnets weren't quite mainstream. You can download the original dataset at: https://www.kaggle.com/c/dogs-vs-cats/data (you will need to create a Kaggle account if you don't already have one -- don't worry, the process is painless).\n","\n","The pictures are medium-resolution color JPEGs. They look like this:\n","![animals](https://drive.google.com/uc?id=1Xzd6kyXM06yiuxunE6XPGZh7vtVG0cYa)\n","\n","Unsurprisingly, the cats vs. dogs Kaggle competition in 2013 was won by entrants who used convnets. The best entries could achieve up to 95% accuracy. In our own example, we will get fairly close to this accuracy, even though we will be training our models on less than 10% of the data that was available to the competitors. This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). We will be using a subset of the data containing three subsets: a training set with 1000 samples of each class, a validation set with 500 samples of each class, and finally a test set with 500 samples of each class."]},{"cell_type":"markdown","metadata":{"id":"vmHxwy2tGAkU"},"source":["First, add [this](https://drive.google.com/open?id=1a85IldM96jUcKyPnu22_ZbOmwc2aoFre) folder with images to your Google Drive. It has the cat and dog images for this lab as well as other image datasets we'll use in the course. You can also download the images to your machine if you would like to run the code below in a Jupyter notebook."]},{"cell_type":"code","metadata":{"id":"9o2Qn3zEY_yJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617992480092,"user_tz":240,"elapsed":35908,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}},"outputId":"31663d0b-8d7e-4d42-f6f6-ee3226b6129b"},"source":["# Mount the drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# The first time that you run this cell, you will need to authorize access in your drive. \n","# Go to the link and copy the authorization code"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K1kCvUaQCldj","executionInfo":{"status":"ok","timestamp":1617992480097,"user_tz":240,"elapsed":35911,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Define directories of where the training, validation and test sets reside\n","# Heather's: base_dir = 'drive/My Drive/Teaching/BST 261/2020/Colab Notebooks/In-class examples/Data/cats_dogs_small/'\n","# This is the path to where my files are - your path will be different, something like this:\n","base_dir = 'drive/Data/cats_dogs_small/'\n","\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","test_dir = os.path.join(base_dir, 'test')\n","\n","train_cats_dir = os.path.join(base_dir, 'train/cats')\n","train_dogs_dir = os.path.join(base_dir, 'train/dogs')\n","validation_cats_dir = os.path.join(base_dir, 'validation/cats')\n","validation_dogs_dir = os.path.join(base_dir, 'validation/dogs')\n","test_cats_dir = os.path.join(base_dir, 'test/cats')\n","test_dogs_dir = os.path.join(base_dir, 'test/dogs')"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6mjKgM5Cr7A","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1617992480692,"user_tz":240,"elapsed":36496,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}},"outputId":"92acf952-ed66-4307-8e0f-f31da5d4aa76"},"source":["# Let's check the number of images in each set\n","print('Total training cat images:', len(os.listdir(train_cats_dir)))\n","print('Total training dog images:', len(os.listdir(train_dogs_dir)))\n","print('Total validation cat images:', len(os.listdir(validation_cats_dir)))\n","print('Total validation dog images:', len(os.listdir(validation_dogs_dir)))\n","print('Total test cat images:', len(os.listdir(test_cats_dir)))\n","print('Total test dog images:', len(os.listdir(test_dogs_dir)))"],"execution_count":13,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-98bd1276f2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's check the number of images in each set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total training cat images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total training dog images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dogs_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total validation cat images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_cats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total validation dog images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dogs_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/Data/cats_dogs_small/train/cats'"]}]},{"cell_type":"markdown","metadata":{"id":"nd8_j0HZG4gb"},"source":["### Creating a CNN from scratch\n","We've already built a small CNN for MNIST in the previous question. We will reuse the same general structure: our CNN will be a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n","\n","However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly larger. This serves both to augment the capacity of the network, and to further reduce the size of the feature maps, so that they aren't overly large when we reach the `Flatten` layer. \n","\n","Define a model with 3 `Conv2D` layers with 32, 64 and 128 3x3 filters respectively and `relu` activation function, each followed by a `MaxPooling2D` layer with a pool size of 2. Then add a `Flatten` layer followed by a `Dense` layer with 512 nodes and `relu` activation function. Finally, add an appropriate output layer. "]},{"cell_type":"code","metadata":{"id":"glYFNdILHT_l","executionInfo":{"status":"aborted","timestamp":1617992480687,"user_tz":240,"elapsed":36489,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Define model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuWR2kt5HUZ9"},"source":["We can check out our model architecture using the following `.summary()` command. Here, since we start from inputs of size 150x150 (a somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the `Flatten` layer.\n","\n","Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is decreasing (from 148x148 to 17x17). This is a pattern that you will see in almost all CNNs.\n"]},{"cell_type":"code","metadata":{"id":"7rPAXGiTHUk0","executionInfo":{"status":"aborted","timestamp":1617992480688,"user_tz":240,"elapsed":36488,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGaQK0NlJzzq"},"source":["Now define how to compile the model with loss function, optimizer and accuracy metric."]},{"cell_type":"code","metadata":{"id":"pGKkuXsZJ0MZ","executionInfo":{"status":"aborted","timestamp":1617992480689,"user_tz":240,"elapsed":36486,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gDqr_31LI3z"},"source":["As you know, data should be formatted into appropriately pre-processed floating point tensors before being fed into our network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:\n","\n","1. Read the picture files.\n","2. Decode the JPEG content to RBG grids of pixels.\n","3. Convert these into floating point tensors.\n","4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n","\n","It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image processing helper tools, located at `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` which allows to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we will use here."]},{"cell_type":"code","metadata":{"id":"gh_am-ryLTky","executionInfo":{"status":"aborted","timestamp":1617992480689,"user_tz":240,"elapsed":36457,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","        # This is the target directory\n","        train_dir,\n","        # All images will be resized to 150x150\n","        target_size = (150, 150),\n","        batch_size = 20,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode = 'binary')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size = (150, 150),\n","        batch_size = 20,\n","        class_mode = 'binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbhsvBUAL6k8"},"source":["Fit the model to the data using the generator. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely, like ours does. Because the data is being generated endlessly, the generator needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the `steps_per_epoch` argument: after having drawn `steps_per_epoch` batches from the generator, i.e. after having run for `steps_per_epoch` gradient descent steps, the fitting process will go to the next epoch.\n","\n","When using the generator, one may pass a `validation_data` argument, much like with the fit method. Importantly, this argument is allowed to be a data generator itself, but it could be a tuple of numpy arrays as well. If you pass a generator as `validation_data`, then this generator is expected to yield batches of validation data endlessly, and thus you should also specify the `validation_steps` argument, which tells the process how many batches to draw from the validation generator for evaluation. Train the model for 15 epochs."]},{"cell_type":"code","metadata":{"id":"NKNb13XgMZJ4","executionInfo":{"status":"aborted","timestamp":1617992480690,"user_tz":240,"elapsed":36456,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FvCLlL0M6H-"},"source":["Plot the accuracy and loss for the train and validation sets."]},{"cell_type":"code","metadata":{"id":"vNq3tOGlM68T","executionInfo":{"status":"aborted","timestamp":1617992480690,"user_tz":240,"elapsed":36444,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["import seaborn as sns\n","sns.set()\n","\n","train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_acc) + 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nryLtH6JM-AF","executionInfo":{"status":"aborted","timestamp":1617992480691,"user_tz":240,"elapsed":36443,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0iSQronM-Ln","executionInfo":{"status":"aborted","timestamp":1617992480691,"user_tz":240,"elapsed":36440,"user":{"displayName":"Gopal Kotecha","photoUrl":"","userId":"05111069707942273767"}}},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yK6kW5GTsfHh"},"source":["Does the model appear to be overfitting? \n","\n"]}]}