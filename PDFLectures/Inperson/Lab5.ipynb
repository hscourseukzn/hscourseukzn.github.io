{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNTfPOFbbl6cxX5AG1d3yu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7WsLZDoex-5H"},"source":["# Lab #5\n","\n","In today's lab we'll extend what we've learned so far on RNNs:\n","\n","  1. Learning our own word embeddings\n","  2. Improving performance by using `LSTM` and `GRU` layers\n","  3. Mitigating overfitting by adding recurrent dropout\n","  4. Improving performance by adding more layers\n","  5. Improving performance by using a `Bidirectional` layer\n"]},{"cell_type":"code","metadata":{"id":"kI4yi01Ox6a_"},"source":["# Import needed packages\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import os\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras import backend as K\n","from keras import datasets, layers, models, preprocessing\n","from keras.preprocessing import sequence\n","from keras.datasets import reuters\n","from keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pySVcw8MZo5c"},"source":["Today we'll be working with the [Reuters dataset,](https://faroit.com/keras-docs/1.2.2/datasets/#reuters-newswire-topics-classification) a set of short newswires and their topics, published by Reuters in 1986 that is built-in to Keras and can be loaded similarly to the IMDb dataset. This dataset contains 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDb dataset, each wire is encoded as a sequence of word indexes and each label is an integer number from 1 to 46.\n","\n","Use the code below to load the data. Here we have chosen the number of words in our dictionary to be 10,000 (just like we did with the IMDb dataset) and the maximum length of a message to be 500 words. Any newswires shorter than 500 words will be padded with 0s and any newswires longer than 500 words will be truncated to the first 500 words. We use the `to_categorical` function to transform the y values from a single interger between 1 and 46 (indicating which of the 46 classes the message has been labeled as) to a vector of length 46 with a 1 in the position associated with the integer label and the rest 0s. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAANPpgy0_Jn","executionInfo":{"status":"ok","timestamp":1619572887309,"user_tz":240,"elapsed":1873,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"93eca001-b11c-4a17-a1c2-370749148d2e"},"source":["# Load data\n","(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words = 10000)\n","\n","x_train = sequence.pad_sequences(x_train, maxlen = 500)\n","x_test = sequence.pad_sequences(x_test, maxlen = 500)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n","\n","# One hot encode labels\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n","2113536/2110848 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"},{"output_type":"stream","text":["x_train shape: (8982, 500)\n","x_test shape: (2246, 500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gWd4soE0nS3Z"},"source":["## Question 1\n","\n","Create an RNN with an `Embedding` layer, 2 `SimpleRNN` layers, and an output layer. Remember that you are performing multiclass classification with 46 total classes. You can choose the number of nodes for each layer and the length of the embedding vector."]},{"cell_type":"code","metadata":{"id":"ei5gHIDeyUgr"},"source":["model = keras.Sequential([\n","\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QlntZgnnzz1"},"source":["Compile and train the model for 10 `epochs` with a `batch_size` of 128 and a `validation_split` of 0.3 (use 30% of the training set for the validation set). "]},{"cell_type":"code","metadata":{"id":"enfzs7X6tjZ6"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Io74e_46oD1I"},"source":["Plot the loss and accuracy for the training and validation sets. \n","Is there evidence of overfitting? "]},{"cell_type":"code","metadata":{"id":"YkD5yDM_uv1y"},"source":["import seaborn as sns\n","\n","train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label = 'Training Loss')\n","plt.plot(epochs, val_loss, label = 'Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUyNh1DEuv5d"},"source":["plt.plot(epochs, train_acc, label = 'Training Accuracy')\n","plt.plot(epochs, val_acc, label = 'Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c49yb_2oU4c"},"source":["## Question 2\n","\n","Now fit the same model as above but with 2 `LSTM` layers instead of `SimpleRNN` layers."]},{"cell_type":"code","metadata":{"id":"LiTBA-touv8p"},"source":["model = keras.Sequential([\n","# Your code here\n","])\n","\n","# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjFSHRZswj5v"},"source":["Plot the loss and accuracy for the training and validation sets. \n","Is there evidence of overfitting? Has the performance of the model improved?"]},{"cell_type":"code","metadata":{"id":"UutEkVopuwAK"},"source":["train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label = 'Training Loss')\n","plt.plot(epochs, val_loss, label = 'Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKP6yY0TuwKW"},"source":["plt.plot(epochs, train_acc, label = 'Training Accuracy')\n","plt.plot(epochs, val_acc, label = 'Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fd5DHE1Zo6FI"},"source":["## Question 3\n","\n","Now fit the same model as above but with 2 `GRU` layers rather than 2 `LSTM` layers."]},{"cell_type":"code","metadata":{"id":"jYfOuZgQuwPR"},"source":["model = keras.Sequential([\n","# Your code here\n","])\n","\n","# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iolHu-vxFF8"},"source":["Plot the loss and accuracy for the training and validation sets. \n","Is there evidence of overfitting? Has the performance of the model improved?\n"]},{"cell_type":"code","metadata":{"id":"6QJuDisxvyaF"},"source":["train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label = 'Training Loss')\n","plt.plot(epochs, val_loss, label = 'Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGrxJr0Dvyf-"},"source":["plt.plot(epochs, train_acc, label = 'Training Accuracy')\n","plt.plot(epochs, val_acc, label = 'Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wShBHQfgo8ji"},"source":["## Question 4\n","\n","Using the same model as in question 3, try adding `dropout` and `recurrent_dropout` arguments to the first `GRU` layer. Chose whatever level of dropout you prefer for each type of dropout. "]},{"cell_type":"code","metadata":{"id":"79KIcLshvyo1"},"source":["model = keras.Sequential([\n","# Your code here\n","])\n","\n","# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCFSH3bDx0HY"},"source":["Plot the loss and accuracy for the training and validation sets. \n","Is there evidence of overfitting? Has the performance of the model improved?\n"]},{"cell_type":"code","metadata":{"id":"96EMhS7-vys8"},"source":["train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label = 'Training Loss')\n","plt.plot(epochs, val_loss, label = 'Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-Vbm9PgvyxJ"},"source":["plt.plot(epochs, train_acc, label = 'Training Accuracy')\n","plt.plot(epochs, val_acc, label = 'Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkNXVmyOyAiJ"},"source":["## Question 5\n","\n","Let's try to improve accuracy by increasing the complexity of the model. Add a third `GRU` layer to your model from Question 4 and train the model."]},{"cell_type":"code","metadata":{"id":"YuFeTI_oyMUd"},"source":["model = keras.Sequential([\n","# Your code here\n","])\n","\n","# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ik8fk28jyXgV"},"source":["Plot the loss and accuracy for the training and validation sets. \n","Is there evidence of overfitting? Has the performance of the model improved?\n"," "]},{"cell_type":"code","metadata":{"id":"WlR7Ye_oyX-P"},"source":["train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label = 'Training Loss')\n","plt.plot(epochs, val_loss, label = 'Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiWmMgJLyYLL"},"source":["plt.plot(epochs, train_acc, label = 'Training Accuracy')\n","plt.plot(epochs, val_acc, label = 'Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QlCuxOYHo-5M"},"source":["## Question 6\n","\n","Let's see if using a bidirectional RNN will increase performance. Create a model similar to your `LSTM` model in Question 2, but with 1 bidirectional LSTM later. "]},{"cell_type":"code","metadata":{"id":"OuSyo_cLvy1J"},"source":["model = keras.Sequential([\n","# Your code here\n","])\n","\n","# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LtIbqmT4y2-f"},"source":["Plot the loss and accuracy for the training and validation sets. \n","Is there evidence of overfitting? Has the performance of the model improved?\n"]},{"cell_type":"code","metadata":{"id":"O0lsXZWk36Je"},"source":["train_acc  = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc  = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label = 'Training Loss')\n","plt.plot(epochs, val_loss, label = 'Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dN0mJUBQ36PQ"},"source":["plt.plot(epochs, train_acc, label = 'Training Accuracy')\n","plt.plot(epochs, val_acc, label = 'Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BdP8MAogy47T"},"source":["## Question 7\n","\n","Which model would you choose for this task? Why did you choose that model?\n"]}]}