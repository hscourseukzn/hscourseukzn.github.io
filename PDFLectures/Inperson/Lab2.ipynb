{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xa7m03wXDqPr"},"source":["# Lab #2\n","\n","Today's lab consists of practice questions to review MLPs and introduce CNNs."]},{"cell_type":"markdown","metadata":{"id":"hejoZsPFRjck"},"source":["## Question 1\n","### Classifying newswires: a multi-class classification example\n","In this example we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Examples include cocoa, soybeans and livestock. Since we have many classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem.\n","\n","### The Reuters dataset\n","We will be working with the Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n","\n","Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look:"]},{"cell_type":"code","metadata":{"id":"F21qC4YGaD_c"},"source":["# Import needed packages\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow import keras\n","import pandas as pd\n","from tensorflow.keras.datasets import reuters\n","from keras.utils import to_categorical\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bj-8a5pSHrP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616939167144,"user_tz":240,"elapsed":1592,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"f3b79f86-cd37-463b-d766-087ee956fe6a"},"source":["#Load data\n","(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n","2113536/2110848 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"IeKpfUp4SM7R"},"source":["\n","Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the data.\n","\n","We have 8,982 training examples and 2,246 test examples:"]},{"cell_type":"code","metadata":{"id":"20B91L0uSQ_N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616939168445,"user_tz":240,"elapsed":355,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"7f3663ab-d688-4892-89b1-b4544cf32e39"},"source":["len(train_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8982"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"5Jv5QgpnSTR5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616939169823,"user_tz":240,"elapsed":376,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"921e1067-318a-40f9-9839-75ad87d08913"},"source":["len(test_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2246"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"gyEQIdwGSWiA"},"source":["As with the IMDB reviews, each example is a list of integers (word indices):"]},{"cell_type":"code","metadata":{"id":"39RJvzpHSWq5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616939171707,"user_tz":240,"elapsed":831,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"6960c841-7b72-4e9a-c94d-766ef78249dc"},"source":["train_data[10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1,\n"," 245,\n"," 273,\n"," 207,\n"," 156,\n"," 53,\n"," 74,\n"," 160,\n"," 26,\n"," 14,\n"," 46,\n"," 296,\n"," 26,\n"," 39,\n"," 74,\n"," 2979,\n"," 3554,\n"," 14,\n"," 46,\n"," 4689,\n"," 4329,\n"," 86,\n"," 61,\n"," 3499,\n"," 4795,\n"," 14,\n"," 61,\n"," 451,\n"," 4329,\n"," 17,\n"," 12]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"C0OquFanSbQF"},"source":["Here's how you can decode it back to words, in case you are curious:"]},{"cell_type":"code","metadata":{"id":"V4iPPKqRSbZ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616939175827,"user_tz":240,"elapsed":831,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"626729fa-6876-41df-dc9f-72eb9ebf93b3"},"source":["word_index = reuters.get_word_index()\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","# Note that our indices were offset by 3\n","# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n","decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n","557056/550378 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7m3ibg4OShQN","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1616939177380,"user_tz":240,"elapsed":587,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"48f7a5c5-ddca-40d3-ef9f-61478d8d3673"},"source":["decoded_newswire"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"xQcVuTS4SksC"},"source":["The label associated with an example is an integer between 0 and 45: a topic index."]},{"cell_type":"code","metadata":{"id":"a1tuutnKSkzc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616939179088,"user_tz":240,"elapsed":382,"user":{"displayName":"Heather Mattie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgT5NLP3MMl4cY_62N9uRkRg1H7U8ExG1x7v3-Ocg=s64","userId":"03574120896352972389"}},"outputId":"d6d22bb1-c636-4451-c83b-487f7529d4fd"},"source":["train_labels[10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"Bgl31PVUUb8D"},"source":["### Preparing the data\n","We can vectorize the data with the exact same code as in our previous example:"]},{"cell_type":"code","metadata":{"id":"SQKJUf4BUdWA"},"source":["def vectorize_sequences(sequences, dimension=10000):\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.\n","    return results\n","\n","# Our vectorized training data\n","x_train = vectorize_sequences(train_data)\n","# Our vectorized test data\n","x_test = vectorize_sequences(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HjfJwkyNUiBA"},"source":["To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1 in [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python). In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"]},{"cell_type":"code","metadata":{"id":"WTo5jwwCUiJl"},"source":["def to_one_hot(labels, dimension=46):\n","    results = np.zeros((len(labels), dimension))\n","    for i, label in enumerate(labels):\n","        results[i, label] = 1.\n","    return results\n","\n","# Our vectorized training labels\n","one_hot_train_labels = to_one_hot(train_labels)\n","# Our vectorized test labels\n","one_hot_test_labels = to_one_hot(test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DP6WFacpUn1J"},"source":["Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"]},{"cell_type":"code","metadata":{"id":"CurEVzaWUn9c"},"source":["one_hot_train_labels = to_categorical(train_labels)\n","one_hot_test_labels = to_categorical(test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkgvTFgEUraG"},"source":["### Building our network\n","This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the dimensionality of the output space is much larger.\n","\n","In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information. For this reason we will use larger layers. \n","\n","Build a networks with 2 hidden layers, each with 64 hidden units, and an appropriate output layer. Remember to include the correct `input_shape` in the first hidden layer."]},{"cell_type":"code","metadata":{"id":"MizNFbsLUrh_"},"source":["# Define model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LranuTl_U10d"},"source":["Code the `model.compile` function with `rmsprop` optimizer, `accuracy` performance metric and appropriate loss function."]},{"cell_type":"code","metadata":{"id":"tdi3F-1wU18H"},"source":["# Define how to execute training\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UjL7xwcU7vM"},"source":["### Validating our approach\n","Let's set apart 1,000 samples in our training data to use as a validation set:"]},{"cell_type":"code","metadata":{"id":"wMTJ3h9QU73v"},"source":["x_val = x_train[:1000]\n","partial_x_train = x_train[1000:]\n","\n","y_val = one_hot_train_labels[:1000]\n","partial_y_train = one_hot_train_labels[1000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4GrP5GtKVENc"},"source":["Train the network for 20 epochs with a batch size of 512. Be sure to make use of the validation set."]},{"cell_type":"code","metadata":{"id":"XPkvsbymVEVf"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5an-LHCrVL0F"},"source":["\n","Display its loss and accuracy curves."]},{"cell_type":"code","metadata":{"id":"P0BQ_2dwVL8t"},"source":["import seaborn as sns\n","sns.set()\n","\n","# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmU5zwtDbs4S"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gN_6FsOBVRNG"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5jqIjbAVXJj"},"source":["It seems that the network starts overfitting after 8 epochs. Train a new network from scratch for 8 epochs, then evaluate it on the test set."]},{"cell_type":"code","metadata":{"id":"vPa6z15XVXRX"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck2aDq1HVghr"},"source":["results = model.evaluate(x_test, one_hot_test_labels)\n","results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAqekri9VkVV"},"source":["Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"]},{"cell_type":"code","metadata":{"id":"rPbrANxJVkwh"},"source":["import copy\n","\n","test_labels_copy = copy.copy(test_labels)\n","np.random.shuffle(test_labels_copy)\n","float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACx1ah7ZVrZb"},"source":["### On the importance of having sufficiently large intermediate layers\n","We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden units. See what happens when you introduce an information bottleneck by setting the number of nodes in the second hidden layer to 4."]},{"cell_type":"code","metadata":{"id":"iMc7LyNpV6M7"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_5ACNQHVgqk"},"source":["Our network now seems to peak at ~66% test accuracy, a 12% absolute drop. This drop is mostly due to the fact that we are now trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram most of the necessary information into these 8-dimensional representations, but not all of it.\n","\n","### Further experiments\n","1. Try using larger or smaller layers: 32 units, 128 units...\n","2. We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."]},{"cell_type":"markdown","metadata":{"id":"HdEQ4utLNWYz"},"source":["# Introduction to CNNs\n","## Question 2: Classifying Digits\n","Let's revisit the MNIST dataset and build a CNN classifier.\n","\n","Load the MNIST dataset provided by keras. This contains 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. Split the data into training and testing sets."]},{"cell_type":"code","metadata":{"id":"7lW91M78Nh1G"},"source":["# Load data\n","from tensorflow.keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ivhL6kMN2AF"},"source":["Print the shape of the training and test sets."]},{"cell_type":"code","metadata":{"id":"M73qe9-uNqi3"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fjbkbSbN99m"},"source":["Let's reshape the data to fit the keras format. Don't worry too much about this chunk for now."]},{"cell_type":"code","metadata":{"id":"S_hgRP9vN-H9"},"source":["from keras import backend as K\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n","    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n","    input_shape = (1, 28, 28)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","    input_shape = (28, 28, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bok_81bON-Qb"},"source":["Now print the shape again to see what changed."]},{"cell_type":"code","metadata":{"id":"u_nhHRULN-Z1"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJ6rhLoFOJEn"},"source":["Question 2 in Homework 1 asks you to train a neural network on the Boston housing data. This dataset contains features on very different scales (for example there are both binary features and real-valued features). While the MNIST features take on values between 0 and 1 and do not need to be normalized, we will go through the exercise of normalizing the values before training our network.\n","\n","Can you think of other algorithms in which normalization is necessary? Is it necessary in the case of neural networks? Why or why not?\n","\n","- Clustering, PCA, random forest, etc. Not necessary (universal function approximator) but makes training easier in cases in which the features have very different scales. \n","\n","\n","Normalize the data. Be sure to normalize the test set with the training set mean and standard deviation. Don't forget to convert the training and testing sets to `float32`."]},{"cell_type":"code","metadata":{"id":"-jPrAAxCOMOi"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mT9w8LNOS4s"},"source":["How will the code above need to be changed for Boston housing dataset? Why?\n","\n","- Need to calculate mean and standard deviation per feature, thus need to use something like `x_train.mean(axis=0)`.\n","\n","Before we define and fit our model let's one-hot encode the labels. Don't forget to do the same for the testing labels and note you will not need to do this step in the case of regression."]},{"cell_type":"code","metadata":{"id":"4zytqKN1OTBc"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpMQ9VuuOiD_"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F751KKZvOTIb"},"source":["Now fit a shallow convolutional neural network with two `Dense` layers. Include one convolutional layer with 32 convolutional filters of size 3x3 and use the `relu` activation function.\n","\n","After the convolutional layer, flatten the tensor to be fed into the `Dense` layers.\n","\n","Add a `Dense` layer with 64 hidden nodes and `relu` activation function.\n","\n","In the output `Dense` layer use enough output nodes to have one corresponding to each class label (10). What is the activation function you should use here?\n","\n","In the optimizer use the `Adadelta` optimization function, and choose an appropriate loss function and model performance measure.\n","\n","Run the network for 5 epochs and use a `batch_size` of 64."]},{"cell_type":"code","metadata":{"id":"PwOybXCaOTQI"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LEqXOKjjOTXq"},"source":["Report the test set accuracy."]},{"cell_type":"code","metadata":{"id":"8dJTnuMKO50z"},"source":["# Your code here"],"execution_count":null,"outputs":[]}]}