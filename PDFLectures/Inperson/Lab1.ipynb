{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aGnMtz5kWezG"},"source":["# Lab 1\n","Today's lab consists of practice questions to review the topics presented thus far in class. We will be focusing on:\n","1. Neural network terminology and architecture\n","2. Python\n","3. Forward and backward propagation\n","4. Tensorflow\n","\n","### Question 1\n","Let's review the terminology introduced by thinking about how to design a model for each the following scenarios. It's important to remember that while there is more than one correct answer in these cases, we want to develop an intuition to help save time in parameter tuning, training, computational resources, etc. We'll also briefy touch on some advanced topics to provide a foundation for later in the course, and remember you do not need to use a deep neural network in every case.\n","\n","*Case 1:* The input is the MNIST handwritten digits dataset (features are 28x28 pixel intensities and labels are digits 0-9). We want to predict which digit the image represents and there are only 10 images per category ($N=100$).\n","\n","    -\n","\n","*Case 2:* The identical setup but this time there are thousands of images per category.\n","\n","    -\n","\n","*Case 3:* The identical setup as case 2 but this time images may contain multiple digits or no digits at all.\n","\n","    -\n","\n","*Case 4:*  The covariates are BMI measurements and reported smoking status, the labels are binary denoting cardiovascular disease. Our sample consists of 70 individuals and we want to predict an individuals' health status based on their BMI and smoking status. We are interested in the effect of BMI on cardiovascular disease.\n","\n","    -\n","\n","*Case 5:* The input consists of thousands of images of different animals and we want to classify which animal the image contains.\n","\n","    -\n","\n","*Case 6:* The input consists of thousands of English sentences and we want to predict the next word in the sentences.\n","\n","    -\n","\n","*Case 7:* The input consists of biomarker status for thousands of loci across thousands of individuals (i.e. Ancestry.com). There are no associated labels and we wish to learn about population substructure.\n","\n","    -"]},{"cell_type":"markdown","metadata":{"id":"Dq7zh9wM8zvg"},"source":[]},{"cell_type":"markdown","metadata":{"id":"XdKVc4HJXHra"},"source":["### Question 2\n","\n","Draw the architecture of a neural network satisying the following conditions:\n","\n","1. X is a univariate covariate. We will consider the case when X=5.\n","2. There are two hidden layers. The first consists of two nodes, each with a bias term taking values (-1 and 1, respectively). The weight going to the first node takes value 0.5 and the weight going to the second node takes value -0.5.\n","3. The nodes in hidden layer 1 each use a linear activation function.\n","4. Hidden layer 2 consists of a single node with no bias term and the ReLU activation function. The weight from node 1 in hidden layer 1 is 0.3 and the weight from node 2 in hidden layer 1 is 0.7.\n","5. Hidden layer 2 outputs to a single output node. The bias term for the output node is 0.5 and the weight from hidden layer 2 is 2.\n","6. The loss function to be optimized is squared loss."]},{"cell_type":"markdown","metadata":{"id":"I88E0fszeDnu"},"source":["![network](https://drive.google.com/uc?id=1drR4LqhaigJnw2-fCzQ6EKtlcfPHG5s2)"]},{"cell_type":"markdown","metadata":{"id":"KqhSrFWpZNWr"},"source":["### Question 3\n","Implement a single forward pass of the network described in Question 3. You do not need to implement the network in keras and should instead use numpy operations (either scalar or matrix). Start by defining the weights and input matrices."]},{"cell_type":"code","metadata":{"id":"blT8waJAWbhj"},"source":["# Import necessary packages\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7qaShKYZbAb"},"source":["# Your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMRFFW49Zqea"},"source":["Now perform the forward pass."]},{"cell_type":"code","metadata":{"id":"WNU-liiGYCjt"},"source":["# Your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjccTRMqZxx5"},"source":["And let's print the values."]},{"cell_type":"code","metadata":{"id":"uBbrhmMxZ00o"},"source":["print('The values for the hidden layer 1 are:', hidden1)\n","print('The values for the hidden layer 2 are:', hidden2)\n","print('The post-relu values for the hidden layer 2 are:', hidden2_clamped)\n","print('The value for the output layer is:', y_hat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cvci-2MiZ3Vg"},"source":["Calculate the loss for the training example given a label of Y = 0.25."]},{"cell_type":"code","metadata":{"id":"Dtmec9jLZ8Wi"},"source":["# Your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJ8qWoFFaDpF"},"source":["Implement a single backward pass of the network. Again use numpy. Start by defining the individual gradient terms."]},{"cell_type":"code","metadata":{"id":"ZCP7P1JxaEIP"},"source":["#gradient for loss\n","  # gradient of loss wrt predicted probability (1x1)\n","\n","# gradients for output layer\n","  # gradient of y_hat wrt hidden output (1x1)\n","  # gradient of  y_hat wrt output layer weight (1x1)\n","  # gradient of  y_hat wrt output layer bias (1x1)\n","\n","# gradient (gate) for relu\n","  # gradient for relu\n","\n","# gradients for second hidden layer\n","  # gradient of second hidden layer wrt second hidden layer weights\n","  # gradient of second hidden layer wrt first hidden layer output\n","\n","# gradients for first hidden layer\n","  # gradient of first hidden layer wrt first hidden layer weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heDVm2xfilUn"},"source":["# gradient of loss wrt output weights\n","# gradient of loss wrt output bias\n","# gradient of loss wrt second hidden layer weights\n","# gradient of loss wrt first hidden layer weights (node 1)\n","# gradient of loss wrt first hidden layer weights (node 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FsIdzUbEaNNR"},"source":["\n","What is the purpose of the partial derivative w.r.t. the weight/parameter\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"9jHurfB0ijSz"},"source":["\n","    -\n","    -"]}]}